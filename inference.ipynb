{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inference Notebook for Persian VITS (Single Speaker - Amir)\n",
    "# Setup environment\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "sys.path.append(\"vits\")\n",
    "\n",
    "import commons\n",
    "import utils\n",
    "from models import SynthesizerTrn\n",
    "from text import text_to_sequence\n",
    "from text.cleaners_fa import persian_cleaners\n",
    "from text.symbols_fa import symbols\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load config & model checkpoint\n",
    "# Change paths as needed\n",
    "checkpoint_path = \"logs/fa_amir/G_10000.pth\"   # replace with latest G_*.pth\n",
    "config_path = \"configs/fa_single_speaker.json\"\n",
    "\n",
    "hps = utils.get_hparams_from_file(config_path)\n",
    "\n",
    "net_g = SynthesizerTrn(\n",
    "    len(symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model\n",
    ").to(device)\n",
    "\n",
    "_ = net_g.eval()\n",
    "\n",
    "_ = utils.load_checkpoint(checkpoint_path, net_g, None)\n",
    "\n",
    "# Text processing (Persian)\n",
    "def infer_text(text, noise_scale=0.667, length_scale=1.0):\n",
    "    \"\"\"Convert Persian text to sequence and synthesize speech.\"\"\"\n",
    "    text = persian_cleaners(text)\n",
    "    seq = text_to_sequence(text, hps.data.text_cleaners)\n",
    "    x = torch.LongTensor(seq).unsqueeze(0).to(device)\n",
    "    x_lengths = torch.LongTensor([x.size(1)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_hat, *_ = net_g.infer(x, x_lengths, noise_scale=noise_scale, length_scale=length_scale)\n",
    "    return y_hat[0][0].cpu().numpy()\n",
    "\n",
    "# Run inference\n",
    "sample_texts = [\n",
    "    \"Ø³Ù„Ø§Ù…! Ø­Ø§Ù„ Ø´Ù…Ø§ Ú†Ø·ÙˆØ±Ù‡ØŸ\",\n",
    "    \"Ø§Ù…Ø±ÙˆØ² ÛŒÚ© Ø±ÙˆØ² Ø®ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„ Ú¯ÙØªØ§Ø± Ø§Ø³Øª.\",\n",
    "    \"Ø§ÛŒÙ† ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ ØµØ¯Ø§ Ø§Ø² Ù…Ø¯Ù„ ÙˆÛŒâ€ŒØ¢ÛŒâ€ŒØªÛŒâ€ŒØ§Ø³ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª.\"\n",
    "]\n",
    "\n",
    "for i, txt in enumerate(sample_texts, 1):\n",
    "    print(f\"ğŸ“ Text {i}: {txt}\")\n",
    "    audio = infer_text(txt)\n",
    "    display(ipd.Audio(audio, rate=hps.data.sampling_rate))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
